---
title: "TeamD_Case3"
author: "Shine,Tianyu Zhang, Lingyi Kong, Zhechen Meng"
date: "3/20/2018"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
#load the package
library(GGally)
library(factoextra)
```

## Part One: Models without LifeLadder
```{r, echo=FALSE,warning=FAlSE}
#read the data
whr_2017DB <- read.csv("whr_2017.csv")
summary(whr_2017DB)

#checking total missing data
sapply(whr_2017DB,function(x) sum(is.na(x)))

#visualize the correlation between variables
ggcorr(whr_2017DB[,-1], label=TRUE, cex=3)

#deal with the missing data,as kmeans can't handle data has NA values
#impute with median
whr_2017DB$LnGDPpc[is.na(whr_2017DB$LnGDPpc)] = median(whr_2017DB$LnGDPpc, na.rm = TRUE)
whr_2017DB$LifeExp[is.na(whr_2017DB$LifeExp)] = median(whr_2017DB$LifeExp, na.rm = TRUE)
whr_2017DB$LifeChoice[is.na(whr_2017DB$LifeChoice)] = median(whr_2017DB$LifeChoice, na.rm = TRUE)
whr_2017DB$Generosity[is.na(whr_2017DB$Generosity)] = median(whr_2017DB$Generosity, na.rm = TRUE)
whr_2017DB$Corruption[is.na(whr_2017DB$Corruption)] = median(whr_2017DB$Corruption, na.rm = TRUE)
whr_2017DB$GDPpc[is.na(whr_2017DB$GDPpc)] = median(whr_2017DB$GDPpc, na.rm = TRUE)

#drop the LifeLaddar and LnGDPpc column
whr_2017_DF <- whr_2017DB[,-c(2:3)]

```
###GDPpc and LnGDPpc are highly correlated, as they represent the same variable, so dorp one for following analysis.

##1.Build the clusters by using hierarchical and k-means
```{r}
# Normalized the data
whr_2017_DF.scaled<- scale(whr_2017_DF[,-1])

#set row names
row.names(whr_2017_DF.scaled) <-whr_2017_DF$country

#build k-means model
par(mfrow = c(1, 1))

# Initialize total within sum of squares error: wss
wss <- 0

# Look over 3 to 8 possible clusters
for (i in 1:8) {
  # Fit the model: km.out
  km.out <- kmeans(whr_2017_DF.scaled, centers = i, nstart = 20, iter.max = 50)
  # Save the within cluster sum of squares
  wss[i] <- km.out$tot.withinss
}

# Produce a scree plot
plot(1:8, wss, type = "b", 
     xlab = "Number of Clusters", 
     ylab = "Within groups sum of squares")

# Select number of clusters
k <- 3

# Build model with k clusters: km.out
km.out <- kmeans(whr_2017_DF.scaled, centers = k, nstart = 50, iter.max = 50)

# View the resulting model
km.out

#plotting profile plot of centroids 
# plot an empty scatter plot
plot(c(0), xaxt = 'n', ylab = "", xlab ="", type = "l",
     ylim = c(min(km.out$centers), max(km.out$centers)), xlim = c(0, 6))

# label x-axes
axis(1, at = c(1:6), labels = colnames(whr_2017_DF.scaled), las = 2)

# plot centroids
for (i in c(1:3))
lines(km.out$centers[i,], lty = i, lwd = 2, col = ifelse(i %in% c(1, 3, 5),
                               "black", "dark grey"))
#name clusters
text(x = 0.5, y = km.out$centers[, 1], labels = paste("Cluster", c(1:3)))

# Create hierarchical clustering model: hclust.out
hclust.out <- hclust(dist(whr_2017_DF.scaled), method="complete")

# Inspect the result
summary(hclust.out)

#prune the tree
hclust.out_cut <- cutree(hclust.out, k = 3)

#inspect the clusters
hclust.out_cut

#get size of each cluster
table(hclust.out_cut)

#plot the heatmap
# set labels as cluster membership and utility name
row.names(whr_2017_DF.scaled) <- paste(hclust.out_cut, ": ", row.names(whr_2017_DF.scaled), sep = "")

# plot heatmap
# rev() reverses the color mapping to large = dark
heatmap(as.matrix(whr_2017_DF.scaled), Colv = NA, hclustfun = hclust,
        col=rev(paste("gray",1:99,sep="")))

#visualize each cluster for hierarchical clustering
fviz_cluster(list(data = whr_2017_DF.scaled, cluster = hclust.out_cut))

```
###Results:
####For K-means model, there are three clusters, and the size for each cluster is 82, 20, 39; the five countries for each cluster are:
*cluster1:Australia,Denmark,Canada,Finland,Germany 
*cluster2:France,Egypt,Argentina,Nepal,Poland
*cluster3:Afghanistan,Ethiopia,Uganda,Zambia,Pakistan
####For hierarchical model, when cluster k=3, the size for each cluster is 5,115,21;the five countries for each cluster are:
*cluster1:Central African Republic,Afghanistan,South Sudan,Haiti,Chad 
*cluster2:Argentina,Brazil,Chile,China,Greece  
*cluster3:Ireland,Iceland,Denmark,Luxembourg,Netherlands
####Discussion:

## Part Two: Models with LifeLadder
```{r}
#add LifeLadder column
whr_2017_DF_LL <- whr_2017DB[,-3]

# Normalized the data
whr_2017_DF_LL.scaled<- scale(whr_2017_DF_LL[,-1])

#set row names
row.names(whr_2017_DF_LL.scaled) <-whr_2017_DF$country

#build k-means model
par(mfrow = c(1, 1))

# Initialize total within sum of squares error: wss
wss <- 0

# Look over 3 to 8 possible clusters
for (i in 1:8) {
  # Fit the model: km.out
  km.out_LL <- kmeans(whr_2017_DF_LL.scaled, centers = i, nstart = 20, iter.max = 50)
  # Save the within cluster sum of squares
  wss[i] <- km.out_LL$tot.withinss
}

# Produce a scree plot
plot(1:8, wss, type = "b", 
     xlab = "Number of Clusters", 
     ylab = "Within groups sum of squares")

# Select number of clusters
k <- 3

# Build model with k clusters: km.out
km.out_LL <- kmeans(whr_2017_DF_LL.scaled, centers = k, nstart = 50, iter.max = 50)

# View the resulting model
km.out_LL

#plotting profile plot of centroids 
# plot an empty scatter plot
plot(c(0), xaxt = 'n', ylab = "", xlab = "", type = "l",
     ylim = c(min(km.out_LL$centers), max(km.out_LL$centers)), xlim = c(0, 7))

# label x-axes
axis(1, at = c(1:7), labels = colnames(whr_2017_DF_LL.scaled), las = 2)

# plot centroids
for (i in c(1:3))
lines(km.out_LL$centers[i,], lty = i, lwd = 2, col = ifelse(i %in% c(1, 3, 5),
                               "black", "dark grey"))
#name clusters
text(x = 0.5, y = km.out_LL$centers[, 1], labels = paste("Cluster", c(1:3)))

# Create hierarchical clustering model: hclust.out
hclust.out_LL <- hclust(dist(whr_2017_DF_LL.scaled), method="complete")

# Inspect the result
summary(hclust.out_LL)

#prune the tree
hclust.out_cut_LL <- cutree(hclust.out_LL, k = 3)

#inspect the clusters
hclust.out_cut_LL

#get size of each cluster
table(hclust.out_cut_LL)

#plot the heatmap
# set labels as cluster membership and utility name
row.names(whr_2017_DF_LL.scaled) <- paste(hclust.out_cut_LL, ": ", row.names(whr_2017_DF_LL.scaled), sep = "")

# plot heatmap
# rev() reverses the color mapping to large = dark
heatmap(as.matrix(whr_2017_DF_LL.scaled), Colv = NA, hclustfun = hclust,
        col=rev(paste("gray",1:99,sep="")))

#visualize each cluster for hierarchical clustering
fviz_cluster(list(data = whr_2017_DF_LL.scaled, cluster = hclust.out_cut_LL))
```
###Results:
####For K-means model, there are three clusters, and the size for each cluster is 73, 20, 48; the five countries for each cluster are:
*cluster1:Brazil, Bolivia, China, France, Philippines
*cluster2:Australia, Belgium, Norway, Netherlands, Luxembourg
*cluster3:Afghanistan, India, Haiti, Indonesia, Ethiopia
####For hierarchical model, when cluster k=3, the size for each cluster is 5,115,21;the five countries for each cluster are:
*cluster1:Afghanistan, Haiti, Chad, South Sudan,Central African Republic
*cluster2:Belgium, Brazil,Cambodia, Chile, China
*cluster3:Australia, United Kingdom, United Arab Emirates, Ireland, Israel 
####Discussion:


##Part 3:Imapct of Happiness Life Ladder 


##Part 4:Explain Clusters


##Part 5:Finally, is it accurate to generalize Tolstoyâ€™s assertion to countries as well as families? 



###Reference
[1].https://rpubs.com/mohammadshadan/273129
[2].https://www.kaggle.com/unsdsn/world-happiness/kernels
[3]https://stat.ethz.ch/R-manual/R-devel/library/stats/html/kmeans.html
[4]http://uc-r.github.io/hc_clustering
[5]Data Mining for Business Analytics
